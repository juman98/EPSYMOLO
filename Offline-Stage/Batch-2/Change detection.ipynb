{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa52464-605d-4e91-b2f0-7d300e466d13",
   "metadata": {},
   "source": [
    "## Import Historical Data from Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e7f625-394b-4f73-9cb2-cbdde5c86ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 702/702 [00:01<00:00, 596.19doc/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_data_in_batches_with_progress(mongo_uri, db_name, collection_name, username, password, batch_size=100000):\n",
    "    # Format the MongoDB URI with the provided username and password\n",
    "    mongo_uri = f\"mongodb://{username}:{password}@{mongo_uri}\"\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Determine the total number of documents to set up the progress bar\n",
    "    total_documents = collection.count_documents({})\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(total=total_documents, desc='Fetching Data', unit='doc')\n",
    "\n",
    "    data_batches = []  # Store data in batches\n",
    "    for skip in range(0, total_documents, batch_size):\n",
    "        # Use skip and limit to fetch the batch\n",
    "        cursor = collection.find().skip(skip).limit(batch_size)\n",
    "        batch = list(cursor)\n",
    "\n",
    "        # Convert the batch to a DataFrame and add it to the list\n",
    "        batch_df = pd.DataFrame(batch)\n",
    "        data_batches.append(batch_df)\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "    # Combine all batches into a single DataFrame\n",
    "    combined_df = pd.concat(data_batches, ignore_index=True)\n",
    "\n",
    "    # Close the progress bar and clean up\n",
    "    pbar.close()\n",
    "    cursor.close()\n",
    "    client.close()\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Use this function to fetch data with progress tracking\n",
    "data = fetch_data_in_batches_with_progress('mongodb:27017', 'Epsymolo', 'power_flow_all_data', 'root', 'root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a61430-9cff-42ec-9827-a60788f6a8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id           object\n",
       "Timestamp     object\n",
       "0            float64\n",
       "1            float64\n",
       "2            float64\n",
       "              ...   \n",
       "1911         float64\n",
       "1912         float64\n",
       "1913         float64\n",
       "1914         float64\n",
       "1915         float64\n",
       "Length: 1918, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c05e85-e176-42c3-bb8f-5d7e5eb277d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f45b-9826-405d-aef9-fe9dca681b4a",
   "metadata": {},
   "source": [
    "## Data Preprocessing with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63368ed0-2e5e-4695-b0fc-dcc6ea570d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, hour,expr\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, abs\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968047f8-4533-4ad7-9a9c-dcf3a2ae7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a Pandas DataFrame `df` from MongoDB:\n",
    "df['_id'] = df['_id'].astype(str)\n",
    "\n",
    "# Now convert the Pandas DataFrame to a PySpark DataFrame\n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "# Drop the '_id' column if it's not needed\n",
    "df_spark = df_spark.drop(\"_id\")\n",
    "\n",
    "# Create an array of column names we want to melt, excluding the Timestamp column\n",
    "value_columns = [c for c in df_spark.columns if c != 'Timestamp']\n",
    "\n",
    "# Create a list of column expressions for the stack function\n",
    "stack_exprs = \", \".join([\"'{0}', `{0}`\".format(c) for c in value_columns])\n",
    "\n",
    "# Create the stack expression string\n",
    "stack_expr = \"stack({0}, {1}) as (PowerLineID, PowerFlowValue)\".format(len(value_columns), stack_exprs)\n",
    "\n",
    "# Add the Timestamp column back, and use \"inline\" to convert the structs into rows\n",
    "df_spark = df_spark.selectExpr(\"Timestamp\", stack_expr)\n",
    "\n",
    "# Show the first few rows of the melted DataFrame\n",
    "df_spark.show()\n",
    "# Convert to Pandas DataFrame for scikit-learn\n",
    "df = df_spark.toPandas()\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a7a90-1ecf-488a-9dfc-abc92ea15c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "windowSpec = Window.partitionBy(\"PowerLineID\").orderBy(\"Timestamp\")\n",
    "df_spark = df_spark.withColumn(\"PrevPowerFlowValue\", lag(\"PowerFlowValue\").over(windowSpec))\n",
    "df_spark = df_spark.withColumn(\"PowerChange\", col(\"PowerFlowValue\") - col(\"PrevPowerFlowValue\"))\n",
    "df_spark = df_spark.na.drop()\n",
    "\n",
    "# Calculate Threshold\n",
    "percentile_threshold = 0.75\n",
    "threshold = df_spark.approxQuantile(\"PowerChange\", [percentile_threshold], 0.05)[0]\n",
    "df_spark = df_spark.withColumn(\"IsSignificantChange\", (abs(col(\"PowerChange\")) > threshold).cast(\"integer\"))\n",
    "\n",
    "# Convert to Pandas DataFrame for scikit-learn\n",
    "df_pandas = df_spark.toPandas()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f579bc-fe32-4ce2-8f64-bc9c7729f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Calculate the power change\n",
    "df['PrevPowerFlowValue'] = df.groupby('PowerLineID')['PowerFlowValue'].shift(1)\n",
    "df['PowerChange'] = df['PowerFlowValue'] - df['PrevPowerFlowValue']\n",
    "\n",
    "# Drop rows with nulls that were introduced by the shift operation\n",
    "df = df.dropna()\n",
    "\n",
    "# Calculate Threshold\n",
    "percentile_threshold = 0.75\n",
    "threshold = df['PowerChange'].quantile(percentile_threshold)\n",
    "\n",
    "# Use the percentile-based threshold to define significant changes\n",
    "df['IsSignificantChange'] = (df['PowerChange'].abs() > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657250c1-541e-41c9-a03b-e7a0d6867902",
   "metadata": {},
   "source": [
    "## Train the model for change detection using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86813244-c60e-476c-817a-e2b11c7beba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Assuming df_pandas is your Pandas DataFrame from the previous steps\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[['PowerFlowValue', 'PowerChange']]  # include other features as needed\n",
    "y = df['IsSignificantChange']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Save the model using joblib\n",
    "joblib.dump(model, 'model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
